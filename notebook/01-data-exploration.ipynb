{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "Initial exploratory data analysis (EDA) and visualization of the legal text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptun_df = pd.read_csv('/data/processed/neptun_data.csv')\n",
    "eval_df = pd.read_csv('/data/processed/evaluation.csv')\n",
    "\n",
    "print(f\"Train data: {len(neptun_df)} samples\")\n",
    "print(f\"Evaluation data: {len(eval_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptun_df['text_length'] = neptun_df['text'].str.len()\n",
    "eval_df['text_length'] = eval_df['text'].str.len()\n",
    "\n",
    "print(\"Train dataset:\")\n",
    "print(neptun_df['text_length'].describe())\n",
    "print(\"\\nEvaluation dataset:\")\n",
    "print(eval_df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptun_df['word_count'] = neptun_df['text'].str.split().str.len()\n",
    "eval_df['word_count'] = eval_df['text'].str.split().str.len()\n",
    "\n",
    "print(\"Train dataset - word count:\")\n",
    "print(neptun_df['word_count'].describe())\n",
    "print(\"\\nEvaluation dataset - word count:\")\n",
    "print(eval_df['word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text complexity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complexity_metrics(text):\n",
    "    \"\"\"Calculate various text complexity metrics\"\"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    sentence_count = len(sentences)\n",
    "    \n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    \n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "    \n",
    "    unique_words = len(set(words))\n",
    "    lexical_diversity = unique_words / word_count if word_count > 0 else 0\n",
    "    \n",
    "    avg_word_length = np.mean([len(w) for w in words]) if words else 0\n",
    "    \n",
    "    punctuation_count = len(re.findall(r'[.,;:!?()\\[\\]{}\"\\'-]', text))\n",
    "    punctuation_ratio = punctuation_count / len(text) if len(text) > 0 else 0\n",
    "    \n",
    "    uppercase_count = sum(1 for c in text if c.isupper())\n",
    "    uppercase_ratio = uppercase_count / len(text) if len(text) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'lexical_diversity': lexical_diversity,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'punctuation_ratio': punctuation_ratio,\n",
    "        'uppercase_ratio': uppercase_ratio\n",
    "    }\n",
    "\n",
    "print(\"Calculating complexity metrics for train dataset...\")\n",
    "complexity_metrics = neptun_df['text'].apply(calculate_complexity_metrics)\n",
    "complexity_df = pd.DataFrame(complexity_metrics.tolist())\n",
    "neptun_df = pd.concat([neptun_df, complexity_df], axis=1)\n",
    "\n",
    "print(\"\\nComplexity metrics summary (train):\")\n",
    "print(complexity_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['avg_sentence_length', 'lexical_diversity', 'avg_word_length', \n",
    "           'punctuation_ratio', 'uppercase_ratio']\n",
    "titles = ['Avg Sentence Length', 'Lexical Diversity', 'Avg Word Length',\n",
    "          'Punctuation Ratio', 'Uppercase Ratio']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    axes[i].hist(neptun_df[metric], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(f'{title} Distribution')\n",
    "    axes[i].set_xlabel(title)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].axvline(neptun_df[metric].median(), color='red', linestyle='--', label='Median')\n",
    "    axes[i].legend()\n",
    "\n",
    "axes[5].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComplexity metrics by label:\")\n",
    "for metric in metrics:\n",
    "    print(f\"\\n{metric.upper()}:\")\n",
    "    print(neptun_df.groupby('label')[metric].agg(['mean', 'std']).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(neptun_df['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Train - text length distribution')\n",
    "axes[0].set_xlabel('Character count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(neptun_df['text_length'].median(), color='red', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(eval_df['text_length'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_title('Evaluation - text length distribution')\n",
    "axes[1].set_xlabel('Character count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(eval_df['text_length'].median(), color='red', linestyle='--', label='Median')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(neptun_df['word_count'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Train - word count distribution')\n",
    "axes[0].set_xlabel('Word count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(neptun_df['word_count'].median(), color='red', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(eval_df['word_count'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_title('Evaluation - word count distribution')\n",
    "axes[1].set_xlabel('Word count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(eval_df['word_count'].median(), color='red', linestyle='--', label='Median')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hungarian_stopwords = set([\n",
    "    'a', 'az', 'egy', 'és', 'vagy', 'de', 'hogy', 'nem', 'van', 'volt',\n",
    "    'lesz', 'lehet', 'mint', 'csak', 'is', 'ha', 'meg', 'el', 'fel',\n",
    "    'ki', 'be', 'le', 'át', 'ezt', 'azt', 'ezen', 'azon', 'amely',\n",
    "    'ami', 'aki', 'ahol', 'amikor', 'ahogy', 'minden', 'semmi', 'valami',\n",
    "    'más', 'egyik', 'másik', 'mindkét', 'több', 'kevés', 'sok', 'néhány',\n",
    "    'által', 'között', 'alatt', 'felett', 'mellett', 'után', 'előtt', 'nélkül',\n",
    "    'miatt', 'számára', 'szerint', 'ellen', 'körül', 'során', 'keresztül',\n",
    "    'illetve', 'valamint', 'továbbá', 'azonban', 'tehát', 'ezért', 'mert'\n",
    "])\n",
    "\n",
    "all_text = ' '.join(neptun_df['text'].values)\n",
    "words = re.findall(r'\\b[a-záéíóöőúüű]+\\b', all_text.lower())\n",
    "\n",
    "word_freq = Counter(words)\n",
    "top_30 = word_freq.most_common(30)\n",
    "\n",
    "print(\"Top 30 most common words:\")\n",
    "for word, count in top_30:\n",
    "    print(f\"{word:20s}: {count:5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_no_stop = [w for w in words if w not in hungarian_stopwords and len(w) > 2]\n",
    "word_freq_no_stop = Counter(words_no_stop)\n",
    "top_30_no_stop = word_freq_no_stop.most_common(30)\n",
    "\n",
    "print(\"Top 30 most common words (without stop words):\")\n",
    "for word, count in top_30_no_stop:\n",
    "    print(f\"{word:20s}: {count:5d}\")\n",
    "\n",
    "words_plot, counts_plot = zip(*top_30_no_stop)\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.barh(range(len(words_plot)), counts_plot, color='steelblue', edgecolor='black')\n",
    "plt.yticks(range(len(words_plot)), words_plot)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 30 most common words (without stop words)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram and trigram analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bigram = CountVectorizer(ngram_range=(2, 2), max_features=30, \n",
    "                                    token_pattern=r'\\b[a-záéíóöőúüű]+\\b')\n",
    "bigram_matrix = vectorizer_bigram.fit_transform(neptun_df['text'].str.lower())\n",
    "bigram_freq = bigram_matrix.sum(axis=0).A1\n",
    "bigram_names = vectorizer_bigram.get_feature_names_out()\n",
    "\n",
    "bigram_data = sorted(zip(bigram_names, bigram_freq), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 30 bigrams:\")\n",
    "for bigram, count in bigram_data[:30]:\n",
    "    print(f\"{bigram:40s}: {count:5.0f}\")\n",
    "\n",
    "bigrams_plot, counts_plot = zip(*bigram_data[:20])\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.barh(range(len(bigrams_plot)), counts_plot, color='coral', edgecolor='black')\n",
    "plt.yticks(range(len(bigrams_plot)), bigrams_plot)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 20 bigrams')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_trigram = CountVectorizer(ngram_range=(3, 3), max_features=30,\n",
    "                                     token_pattern=r'\\b[a-záéíóöőúüű]+\\b')\n",
    "trigram_matrix = vectorizer_trigram.fit_transform(neptun_df['text'].str.lower())\n",
    "trigram_freq = trigram_matrix.sum(axis=0).A1\n",
    "trigram_names = vectorizer_trigram.get_feature_names_out()\n",
    "\n",
    "trigram_data = sorted(zip(trigram_names, trigram_freq), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 30 trigrams:\")\n",
    "for trigram, count in trigram_data[:30]:\n",
    "    print(f\"{trigram:50s}: {count:5.0f}\")\n",
    "\n",
    "trigrams_plot, counts_plot = zip(*trigram_data[:20])\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.barh(range(len(trigrams_plot)), counts_plot, color='lightgreen', edgecolor='black')\n",
    "plt.yticks(range(len(trigrams_plot)), trigrams_plot, fontsize=9)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 20 trigrams')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Analysis by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top TF-IDF words by label:\\n\")\n",
    "\n",
    "for label in sorted(neptun_df['label'].unique()):\n",
    "    label_texts = neptun_df[neptun_df['label'] == label]['text'].values\n",
    "    \n",
    "    tfidf = TfidfVectorizer(max_features=15, token_pattern=r'\\b[a-záéíóöőúüű]{3,}\\b',\n",
    "                            stop_words=list(hungarian_stopwords))\n",
    "    tfidf_matrix = tfidf.fit_transform(label_texts)\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    avg_tfidf = tfidf_matrix.mean(axis=0).A1\n",
    "    \n",
    "    top_words = sorted(zip(feature_names, avg_tfidf), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Label {label}:\")\n",
    "    for word, score in top_words[:10]:\n",
    "        print(f\"  {word:20s}: {score:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, label in enumerate(sorted(neptun_df['label'].unique())):\n",
    "    label_texts = neptun_df[neptun_df['label'] == label]['text'].values\n",
    "    \n",
    "    tfidf = TfidfVectorizer(max_features=10, token_pattern=r'\\b[a-záéíóöőúüű]{3,}\\b',\n",
    "                            stop_words=list(hungarian_stopwords))\n",
    "    tfidf_matrix = tfidf.fit_transform(label_texts)\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    avg_tfidf = tfidf_matrix.mean(axis=0).A1\n",
    "    \n",
    "    top_words = sorted(zip(feature_names, avg_tfidf), key=lambda x: x[1], reverse=True)\n",
    "    words, scores = zip(*top_words)\n",
    "    \n",
    "    axes[idx].barh(range(len(words)), scores, color='skyblue', edgecolor='black')\n",
    "    axes[idx].set_yticks(range(len(words)))\n",
    "    axes[idx].set_yticklabels(words, fontsize=9)\n",
    "    axes[idx].set_xlabel('TF-IDF score')\n",
    "    axes[idx].set_title(f'Label {label} - top TF-IDF words')\n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "axes[5].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text length by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "neptun_df.boxplot(column='text_length', by='label', ax=axes[0])\n",
    "axes[0].set_title('Train - text length by label')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Character count')\n",
    "plt.sca(axes[0])\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "eval_df.boxplot(column='text_length', by='label', ax=axes[1])\n",
    "axes[1].set_title('Evaluation - text length by label')\n",
    "axes[1].set_xlabel('Label')\n",
    "axes[1].set_ylabel('Character count')\n",
    "plt.sca(axes[1])\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Samples', 'Avg text length', 'Avg word count', 'Min length', 'Max length'],\n",
    "    'Train': [\n",
    "        len(neptun_df),\n",
    "        neptun_df['text_length'].mean(),\n",
    "        neptun_df['word_count'].mean(),\n",
    "        neptun_df['text_length'].min(),\n",
    "        neptun_df['text_length'].max()\n",
    "    ],\n",
    "    'Evaluation': [\n",
    "        len(eval_df),\n",
    "        eval_df['text_length'].mean(),\n",
    "        eval_df['word_count'].mean(),\n",
    "        eval_df['text_length'].min(),\n",
    "        eval_df['text_length'].max()\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
