{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Analysis\n",
    "Analyzing the distribution and properties of the target labels (readability scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptun_df = pd.read_csv('/data/processed/neptun_data.csv')\n",
    "eval_df = pd.read_csv('/data/processed/evaluation.csv')\n",
    "\n",
    "print(f\"Train data: {len(neptun_df)} samples\")\n",
    "print(f\"Evaluation data: {len(eval_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Complexity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complexity_metrics(text):\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    sentence_count = len(sentences)\n",
    "    \n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    \n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "    unique_words = len(set(words))\n",
    "    lexical_diversity = unique_words / word_count if word_count > 0 else 0\n",
    "    avg_word_length = np.mean([len(w) for w in words]) if words else 0\n",
    "    punctuation_count = len(re.findall(r'[.,;:!?()\\[\\]{}\"\\'-]', text))\n",
    "    punctuation_ratio = punctuation_count / len(text) if len(text) > 0 else 0\n",
    "    uppercase_count = sum(1 for c in text if c.isupper())\n",
    "    uppercase_ratio = uppercase_count / len(text) if len(text) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'lexical_diversity': lexical_diversity,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'punctuation_ratio': punctuation_ratio,\n",
    "        'uppercase_ratio': uppercase_ratio\n",
    "    }\n",
    "\n",
    "print(\"Calculating complexity metrics...\")\n",
    "complexity_metrics = neptun_df['text'].apply(calculate_complexity_metrics)\n",
    "complexity_df = pd.DataFrame(complexity_metrics.tolist())\n",
    "neptun_df = pd.concat([neptun_df.reset_index(drop=True), complexity_df], axis=1)\n",
    "\n",
    "neptun_df['text_length'] = neptun_df['text'].str.len()\n",
    "neptun_df['word_count'] = neptun_df['text'].str.split().str.len()\n",
    "eval_df['text_length'] = eval_df['text'].str.len()\n",
    "eval_df['word_count'] = eval_df['text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {\n",
    "    1: 'Very Hard',\n",
    "    2: 'Hard',\n",
    "    3: 'Moderate',\n",
    "    4: 'Easy',\n",
    "    5: 'Very Easy'\n",
    "}\n",
    "\n",
    "print(\"Train dataset - label distribution:\")\n",
    "neptun_counts = neptun_df['label'].value_counts().sort_index()\n",
    "for label, count in neptun_counts.items():\n",
    "    pct = (count / len(neptun_df)) * 100\n",
    "    print(f\"  {label} ({label_names[label]}): {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\nEvaluation dataset - label distribution:\")\n",
    "eval_counts = eval_df['label'].value_counts().sort_index()\n",
    "for label, count in eval_counts.items():\n",
    "    pct = (count / len(eval_df)) * 100\n",
    "    print(f\"  {label} ({label_names[label]}): {count:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "neptun_counts.plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Train - label distribution')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels([f\"{l}\\n{label_names[l]}\" for l in neptun_counts.index], rotation=45, ha='right')\n",
    "for i, v in enumerate(neptun_counts.values):\n",
    "    axes[0].text(i, v + 10, str(v), ha='center', va='bottom')\n",
    "\n",
    "eval_counts.plot(kind='bar', ax=axes[1], color='coral', edgecolor='black')\n",
    "axes[1].set_title('Evaluation - label distribution')\n",
    "axes[1].set_xlabel('Label')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticklabels([f\"{l}\\n{label_names[l]}\" for l in eval_counts.index], rotation=45, ha='right')\n",
    "for i, v in enumerate(eval_counts.values):\n",
    "    axes[1].text(i, v + 1, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity vs Label Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix with all complexity metrics\n",
    "complexity_features = ['label', 'text_length', 'word_count', 'avg_sentence_length', \n",
    "                       'lexical_diversity', 'avg_word_length', 'punctuation_ratio', 'uppercase_ratio']\n",
    "\n",
    "correlation_matrix = neptun_df[complexity_features].corr()\n",
    "\n",
    "print(\"Correlation matrix - all features vs lLabel:\")\n",
    "print(correlation_matrix['label'].sort_values(ascending=False))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation matrix: complexity metrics vs label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_to_plot = ['text_length', 'word_count', 'avg_sentence_length', \n",
    "                   'lexical_diversity', 'avg_word_length', 'punctuation_ratio']\n",
    "titles = ['Text Length', 'Word Count', 'Avg Sentence Length',\n",
    "          'Lexical Diversity', 'Avg Word Length', 'Punctuation Ratio']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    axes[idx].scatter(neptun_df[metric], neptun_df['label'], alpha=0.3, s=20)\n",
    "    axes[idx].set_xlabel(title)\n",
    "    axes[idx].set_ylabel('Label')\n",
    "    axes[idx].set_title(f'{title} vs Label')\n",
    "    axes[idx].set_yticks([1, 2, 3, 4, 5])\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    corr = neptun_df[[metric, 'label']].corr().iloc[0, 1]\n",
    "    axes[idx].text(0.05, 0.95, f'r = {corr:.3f}', \n",
    "                   transform=axes[idx].transAxes, \n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "                   verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Fit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "key_metrics = ['text_length', 'lexical_diversity', 'avg_sentence_length', 'avg_word_length']\n",
    "metric_titles = ['Text Length', 'Lexical Diversity', 'Avg Sentence Length', 'Avg Word Length']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(key_metrics, metric_titles)):\n",
    "    X = neptun_df[metric].values.reshape(-1, 1)\n",
    "    y = neptun_df['label'].values\n",
    "    \n",
    "    mask = ~np.isnan(X.flatten())\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "    \n",
    "    axes[idx].scatter(X_clean, y_clean, alpha=0.3, s=20, label='Data')\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_clean, y_clean)\n",
    "    X_range = np.linspace(X_clean.min(), X_clean.max(), 100).reshape(-1, 1)\n",
    "    y_pred_linear = lr.predict(X_range)\n",
    "    r2_linear = r2_score(y_clean, lr.predict(X_clean))\n",
    "    axes[idx].plot(X_range, y_pred_linear, 'r-', linewidth=2, \n",
    "                   label=f'Linear (R²={r2_linear:.3f})')\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_poly = poly.fit_transform(X_clean)\n",
    "    lr_poly = LinearRegression()\n",
    "    lr_poly.fit(X_poly, y_clean)\n",
    "    X_range_poly = poly.transform(X_range)\n",
    "    y_pred_poly = lr_poly.predict(X_range_poly)\n",
    "    r2_poly = r2_score(y_clean, lr_poly.predict(X_poly))\n",
    "    axes[idx].plot(X_range, y_pred_poly, 'g--', linewidth=2,\n",
    "                   label=f'Polynomial (R²={r2_poly:.3f})')\n",
    "    \n",
    "    axes[idx].set_xlabel(title)\n",
    "    axes[idx].set_ylabel('Label')\n",
    "    axes[idx].set_title(f'{title} vs Label - Fit Comparison')\n",
    "    axes[idx].set_yticks([1, 2, 3, 4, 5])\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df, column, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(df[column]))\n",
    "    return z_scores > threshold\n",
    "\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "\n",
    "print(\"Outlier detection by label\\n\" + \"=\"*60)\n",
    "\n",
    "for label in sorted(neptun_df['label'].unique()):\n",
    "    label_data = neptun_df[neptun_df['label'] == label].copy()\n",
    "    \n",
    "    outliers_zscore_length = detect_outliers_zscore(label_data, 'text_length')\n",
    "    outliers_iqr_length = detect_outliers_iqr(label_data, 'text_length')\n",
    "    outliers_zscore_lex = detect_outliers_zscore(label_data, 'lexical_diversity')\n",
    "    outliers_iqr_lex = detect_outliers_iqr(label_data, 'lexical_diversity')\n",
    "    \n",
    "    combined_outliers = outliers_zscore_length | outliers_iqr_length | outliers_zscore_lex | outliers_iqr_lex\n",
    "    \n",
    "    print(f\"\\nLabel {label} ({label_names[label]}):\")\n",
    "    print(f\"  Total samples: {len(label_data)}\")\n",
    "    print(f\"  Outliers (Z-score, text_length): {outliers_zscore_length.sum()}\")\n",
    "    print(f\"  Outliers (IQR, text_length): {outliers_iqr_length.sum()}\")\n",
    "    print(f\"  Outliers (Z-score, lexical_diversity): {outliers_zscore_lex.sum()}\")\n",
    "    print(f\"  Outliers (IQR, lexical_diversity): {outliers_iqr_lex.sum()}\")\n",
    "    print(f\"  Combined outliers: {combined_outliers.sum()} ({combined_outliers.sum()/len(label_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, label in enumerate(sorted(neptun_df['label'].unique())):\n",
    "    label_data = neptun_df[neptun_df['label'] == label].copy()\n",
    "    \n",
    "    outliers_length = detect_outliers_iqr(label_data, 'text_length')\n",
    "    outliers_lex = detect_outliers_iqr(label_data, 'lexical_diversity')\n",
    "    \n",
    "    axes[idx].scatter(label_data.loc[~outliers_length & ~outliers_lex, 'text_length'],\n",
    "                     label_data.loc[~outliers_length & ~outliers_lex, 'lexical_diversity'],\n",
    "                     alpha=0.5, s=30, label='Normal', color='blue')\n",
    "    \n",
    "    axes[idx].scatter(label_data.loc[outliers_length | outliers_lex, 'text_length'],\n",
    "                     label_data.loc[outliers_length | outliers_lex, 'lexical_diversity'],\n",
    "                     alpha=0.8, s=50, label='Outlier', color='red', marker='x')\n",
    "    \n",
    "    axes[idx].set_xlabel('Text length')\n",
    "    axes[idx].set_ylabel('Lexical diversity')\n",
    "    axes[idx].set_title(f'Label {label} - outlier detection (IQR)')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "axes[5].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in sorted(neptun_df['label'].unique()):\n",
    "    label_data = neptun_df[neptun_df['label'] == label].copy()\n",
    "    \n",
    "    outliers_length = detect_outliers_iqr(label_data, 'text_length')\n",
    "    outliers_lex = detect_outliers_iqr(label_data, 'lexical_diversity')\n",
    "    combined_outliers = outliers_length | outliers_lex\n",
    "    \n",
    "    if combined_outliers.sum() > 0:\n",
    "        outlier_samples = label_data[combined_outliers].head(2)\n",
    "        \n",
    "        print(f\"\\nLabel {label} ({label_names[label]}):\")\n",
    "        for idx, row in outlier_samples.iterrows():\n",
    "            print(f\"  Text: {row['text'][:150]}...\")\n",
    "            print(f\"  Length: {row['text_length']}, lex div: {row['lexical_diversity']:.3f}\")\n",
    "            print(f\"  Reason: \", end=\"\")\n",
    "            if outliers_length.loc[idx]:\n",
    "                print(\"Unusual text length \", end=\"\")\n",
    "            if outliers_lex.loc[idx]:\n",
    "                print(\"Unusual lexical diversity\")\n",
    "            print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_imbalance_ratio(df):\n",
    "    counts = df['label'].value_counts()\n",
    "    max_count = counts.max()\n",
    "    min_count = counts.min()\n",
    "    return max_count / min_count\n",
    "\n",
    "neptun_ratio = calculate_imbalance_ratio(neptun_df)\n",
    "eval_ratio = calculate_imbalance_ratio(eval_df)\n",
    "\n",
    "print(f\"Train imbalance ratio: {neptun_ratio:.2f}:1\")\n",
    "print(f\"Evaluation imbalance ratio: {eval_ratio:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "neptun_counts.plot(kind='pie', ax=axes[0], autopct='%1.1f%%', startangle=90,\n",
    "                   labels=[label_names[l] for l in neptun_counts.index])\n",
    "axes[0].set_title('Train - label proportions')\n",
    "axes[0].set_ylabel('')\n",
    "\n",
    "eval_counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', startangle=90,\n",
    "                 labels=[label_names[l] for l in eval_counts.index])\n",
    "axes[1].set_title('Evaluation - label proportions')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(label_names))\n",
    "width = 0.35\n",
    "\n",
    "neptun_pct = (neptun_counts / len(neptun_df) * 100).values\n",
    "eval_pct = (eval_counts / len(eval_df) * 100).values\n",
    "\n",
    "ax.bar(x - width/2, neptun_pct, width, label='Train', color='steelblue', edgecolor='black')\n",
    "ax.bar(x + width/2, eval_pct, width, label='Evaluation', color='coral', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Percentage (%)')\n",
    "ax.set_title('Label distribution comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{l}\\n{label_names[l]}\" for l in sorted(label_names.keys())])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
